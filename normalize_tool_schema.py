#!/usr/bin/env python3
"""
æ ‡å‡†åŒ– tool.txt æ–‡ä»¶ï¼Œä¿®å¤å¸¸è§çš„ JSON æ ¼å¼é—®é¢˜

åŠŸèƒ½ï¼š
1. è¯»å–åŸå§‹ tool.txt
2. å°è¯•è§£ææ¯ä¸ªå·¥å…·çš„ Parameters å’Œ Output
3. ä¿®å¤å¸¸è§æ ¼å¼é—®é¢˜ï¼ˆä¸­æ–‡æ ‡ç‚¹ã€æ‹¬å·ä¸åŒ¹é…ã€å…³é”®å­—é”™è¯¯ç­‰ï¼‰
4. ç”Ÿæˆæ ‡å‡†åŒ–çš„ tool_normalized.txt

ä½¿ç”¨æ–¹æ³•ï¼š
    python normalize_tool_schema.py
"""

import json
import re
import ast
from pathlib import Path
from typing import Dict, Optional, Any, List


def fix_chinese_punctuation(s: str) -> str:
    """æ›¿æ¢ä¸­æ–‡æ ‡ç‚¹ä¸ºè‹±æ–‡æ ‡ç‚¹"""
    replacements = {
        'ï¼Œ': ',',
        'ï¼š': ':',
        'ï¼›': ';',
        '"': '"',
        '"': '"',
        ''': "'",
        ''': "'",
        'ï¼ˆ': '(',
        'ï¼‰': ')',
        'ã€': '[',
        'ã€‘': ']',
        'ã€Š': '<',
        'ã€‹': '>',
    }
    for cn, en in replacements.items():
        s = s.replace(cn, en)
    return s


def fix_properties_closure(s: str) -> str:
    """
    ä¿®å¤ properties å¯¹è±¡ç¼ºå°‘é—­åˆæ‹¬å·çš„é—®é¢˜
    
    åŸå§‹é—®é¢˜ï¼š
    {'properties': {'key1': {...}}, 'key2': {...}, 'required': [...]}
    
    åº”è¯¥ä¿®å¤ä¸ºï¼š
    {'properties': {'key1': {...}, 'key2': {...}}, 'required': [...]}
    """
    # æŸ¥æ‰¾ "properties": { ... çš„ä½ç½®
    props_match = re.search(r'["\']properties["\']\s*:\s*\{', s)
    if not props_match:
        return s
    
    props_start = props_match.end() - 1  # { çš„ä½ç½®
    
    # ä» properties å¼€å§‹ï¼Œæ‰¾åˆ°å¯¹åº”çš„é—­åˆ }
    # åŒæ—¶æ£€æµ‹æ˜¯å¦é‡åˆ°é¡¶å±‚å…³é”®å­—
    depth = 0
    in_string = False
    escape = False
    i = props_start
    
    top_level_keys = ['required', 'type', 'additionalProperties', 'description']
    
    # è®°å½•æœ€åä¸€ä¸ªå¯èƒ½çš„é—­åˆä½ç½®ï¼ˆåœ¨é‡åˆ°é¡¶å±‚å…³é”®å­—ä¹‹å‰ï¼‰
    last_valid_close = -1
    
    while i < len(s):
        c = s[i]
        
        if escape:
            escape = False
            i += 1
            continue
        
        if c == '\\':
            escape = True
            i += 1
            continue
        
        if c in ('"', "'") and not in_string:
            in_string = c
        elif c == in_string:
            in_string = False
        elif not in_string:
            if c == '{':
                depth += 1
            elif c == '}':
                depth -= 1
                if depth == 0:
                    # properties æ­£å¸¸é—­åˆ
                    return s
                elif depth == 1:
                    # è®°å½• properties å†…éƒ¨å±æ€§çš„é—­åˆä½ç½®
                    last_valid_close = i
            elif c == ',' and depth == 1:
                # åœ¨ properties çš„ç›´æ¥å­çº§ï¼Œæ£€æŸ¥åé¢æ˜¯å¦è·Ÿç€é¡¶å±‚å…³é”®å­—
                remaining = s[i+1:].lstrip()
                for key in top_level_keys:
                    if remaining.startswith(f'"{key}"') or remaining.startswith(f"'{key}"):
                        # æ‰¾åˆ°é¡¶å±‚å…³é”®å­—ï¼Œéœ€è¦åœ¨æ­¤ä¹‹å‰é—­åˆ properties
                        # åœ¨æœ€åä¸€ä¸ªæœ‰æ•ˆé—­åˆä½ç½®ä¹‹åæ’å…¥ }
                        if last_valid_close != -1:
                            insert_pos = last_valid_close + 1
                            s = s[:insert_pos] + '}' + s[insert_pos:]
                        return s
        i += 1
    
    return s


def fix_bracket_mismatch(s: str) -> str:
    """
    å°è¯•ä¿®å¤å¸¸è§çš„æ‹¬å·ä¸åŒ¹é…é—®é¢˜
    """
    # ä¿®å¤æ¨¡å¼: å­—ç¬¦ä¸²å€¼åç›´æ¥è·Ÿ ,{ åº”è¯¥æ˜¯ },{
    s = re.sub(r'(["\'])\s*,\s*(\{)', r'\1},\2', s)
    
    return s


def fix_anyof_structure(s: str) -> str:
    """
    ä¿®å¤ anyOf/oneOf/allOf ç»“æ„ä¸­çš„å¸¸è§é—®é¢˜
    
    é—®é¢˜: {'anyof':[{'type':'string',{'type':'null'},'default':None,'title':'X'}]}
    ä¿®å¤: {'anyOf':[{'type':'string'},{'type':'null'}],'default':None,'title':'X'}
    """
    # æŸ¥æ‰¾æ‰€æœ‰ anyOf/oneOf/allOf æ¨¡å¼ï¼ˆåŒ…æ‹¬æœªé—­åˆçš„æ•°ç»„ï¼‰
    pattern = r'(["\'])(anyof|oneof|allof)\1\s*:\s*\[([^\]]+?)(\]|$)'
    
    def fix_array(match):
        quote = match.group(1)
        key = match.group(2)
        array_content = match.group(3)
        has_closing = match.group(4) == ']'
        
        # ä¿®å¤æ•°ç»„å†…çš„ ,{ -> },{
        fixed = re.sub(r'(["\'])\s*,\s*(\{)', r'\1},\2', array_content)
        
        # æå–æ•°ç»„å¤–çš„å±æ€§ï¼ˆdefault, title ç­‰ï¼‰
        outer_props = []
        remaining = fixed
        
        # æŸ¥æ‰¾æ‰€æœ‰åº”è¯¥åœ¨å¤–å±‚çš„å±æ€§
        for prop in ['default', 'title', 'description']:
            # åŒ¹é… ,'prop': value æˆ– ,"prop": value
            prop_pattern = rf',\s*(["\']){prop}\1\s*:\s*([^,\]}}]+)'
            matches = list(re.finditer(prop_pattern, remaining))
            if matches:
                # å–æœ€åä¸€ä¸ªåŒ¹é…
                last_match = matches[-1]
                outer_props.append(last_match.group(0))
                # ä» remaining ä¸­ç§»é™¤
                remaining = remaining[:last_match.start()] + remaining[last_match.end():]
        
        # æ¸…ç† remainingï¼ˆæ•°ç»„å†…å®¹ï¼‰
        # ç§»é™¤å°¾éšçš„ } å¦‚æœå®ƒä¸å±äºæ•°ç»„å…ƒç´ 
        remaining = remaining.rstrip(',').strip()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä½™çš„ }
        # è®¡ç®— { å’Œ } çš„æ•°é‡
        open_count = remaining.count('{')
        close_count = remaining.count('}')
        if close_count > open_count:
            # ç§»é™¤å¤šä½™çš„ }
            for _ in range(close_count - open_count):
                # ä»æœ«å°¾ç§»é™¤æœ€åä¸€ä¸ª }
                last_brace = remaining.rfind('}')
                if last_brace != -1:
                    remaining = remaining[:last_brace] + remaining[last_brace+1:]
        
        # é‡ç»„ï¼Œä¿æŒåŸå§‹å…³é”®å­—å¤§å°å†™
        result = f'{quote}{key}{quote}:[{remaining}]'
        if outer_props:
            result += ''.join(outer_props)
        
        return result
    
    s = re.sub(pattern, fix_array, s, flags=re.IGNORECASE)
    return s


def fix_enum_values(s: str) -> str:
    """
    ä¿®å¤ enum æ•°ç»„ä¸­ç¼ºå°‘å¼•å·çš„å€¼
    
    ä¾‹å¦‚: enum:[complete,partial] -> enum:["complete","partial"]
    """
    # æŸ¥æ‰¾æ‰€æœ‰ enum: [...] æ¨¡å¼
    pattern = r'(["\']enum["\']\s*:\s*\[)([^\]]+)(\])'
    
    def fix_enum_array(match):
        prefix = match.group(1)
        content = match.group(2)
        suffix = match.group(3)
        
        # åˆ†å‰²æ•°ç»„å…ƒç´ 
        items = []
        current = []
        in_string = False
        string_char = None
        
        for c in content:
            if not in_string:
                if c in ('"', "'"):
                    in_string = True
                    string_char = c
                    current.append(c)
                elif c == ',':
                    if current:
                        items.append(''.join(current).strip())
                        current = []
                else:
                    current.append(c)
            else:
                current.append(c)
                if c == string_char:
                    in_string = False
                    string_char = None
        
        if current:
            items.append(''.join(current).strip())
        
        # ä¿®å¤æ¯ä¸ªå…ƒç´ ï¼šå¦‚æœä¸æ˜¯ä»¥å¼•å·å¼€å¤´ï¼Œæ·»åŠ å¼•å·
        fixed_items = []
        for item in items:
            item = item.strip()
            if not item:
                continue
            # å¦‚æœå·²ç»æœ‰å¼•å·ï¼Œä¿æŒä¸å˜
            if item.startswith('"') or item.startswith("'"):
                fixed_items.append(item)
            else:
                # æ·»åŠ å¼•å·
                fixed_items.append(f'"{item}"')
        
        return prefix + ','.join(fixed_items) + suffix
    
    s = re.sub(pattern, fix_enum_array, s, flags=re.IGNORECASE)
    return s


def fix_json_string(s: str) -> str:
    """
    ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
    
    å¤„ç†çš„é—®é¢˜ï¼š
    1. ä¸­æ–‡æ ‡ç‚¹
    2. æœªåŠ å¼•å·çš„key
    3. å•å¼•å·è½¬åŒå¼•å·
    4. Python None/True/False
    5. å°¾éšé€—å·
    6. JSON Schema å…³é”®å­—å¤§å°å†™
    7. æ‹¬å·ä¸åŒ¹é…
    8. anyOf/oneOf/allOf ç»“æ„é”™è¯¯
    9. enum å€¼ç¼ºå°‘å¼•å·
    """
    if not s:
        return s
    
    # æ­¥éª¤1: æ›¿æ¢ä¸­æ–‡æ ‡ç‚¹
    s = fix_chinese_punctuation(s)
    
    # æ­¥éª¤1.5: ä¿®å¤ enum å€¼ç¼ºå°‘å¼•å·
    s = fix_enum_values(s)
    
    # æ­¥éª¤2: ä¿®å¤ properties é—­åˆé—®é¢˜
    s = fix_properties_closure(s)
    
    # æ­¥éª¤3: å°è¯•ä¿®å¤æ‹¬å·ä¸åŒ¹é…
    s = fix_bracket_mismatch(s)
    
    # æ­¥éª¤4: ä¿®å¤ anyOf/oneOf/allOf ç»“æ„
    s = fix_anyof_structure(s)
    
    # æ­¥éª¤3: ä¿®å¤æœªåŠ å¼•å·çš„keyï¼ˆåœ¨ { æˆ– , åé¢çš„æ ‡è¯†ç¬¦åè·Ÿç€ :ï¼‰
    s = re.sub(r'([{,])\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1"\2":', s)
    
    # æ­¥éª¤4: å°†å•å¼•å·è½¬æ¢ä¸ºåŒå¼•å·ï¼ˆå°å¿ƒå¤„ç†å­—ç¬¦ä¸²å†…å®¹ï¼‰
    result = []
    in_string = False
    string_char = None
    i = 0
    while i < len(s):
        c = s[i]
        
        if not in_string:
            if c == '"':
                in_string = True
                string_char = '"'
                result.append(c)
            elif c == "'":
                in_string = True
                string_char = "'"
                result.append('"')  # å•å¼•å·è½¬åŒå¼•å·
            else:
                result.append(c)
        else:
            if c == '\\' and i + 1 < len(s):
                # è½¬ä¹‰å­—ç¬¦
                result.append(c)
                result.append(s[i + 1])
                i += 2
                continue
            elif c == string_char:
                in_string = False
                string_char = None
                result.append('"')  # ç»Ÿä¸€è½¬ä¸ºåŒå¼•å·
            else:
                result.append(c)
        i += 1
    
    s = ''.join(result)
    
    # æ­¥éª¤5: å¤„ç† Python çš„ None, True, False
    s = re.sub(r'\bNone\b', 'null', s)
    s = re.sub(r'\bTrue\b', 'true', s)
    s = re.sub(r'\bFalse\b', 'false', s)
    
    # æ­¥éª¤6: ç§»é™¤å°¾éšé€—å· (,] æˆ– ,})
    s = re.sub(r',\s*([\]\}])', r'\1', s)
    
    # æ­¥éª¤7: ä¸ä¿®å¤ JSON Schema å…³é”®å­—å¤§å°å†™ï¼Œä¿æŒåŸæ ·
    # s = re.sub(r'"anyof":', '"anyOf":', s, flags=re.IGNORECASE)
    # s = re.sub(r'"oneof":', '"oneOf":', s, flags=re.IGNORECASE)
    # s = re.sub(r'"allof":', '"allOf":', s, flags=re.IGNORECASE)
    
    return s


def normalize_quotes(text: str) -> str:
    """
    å°†æ–‡æœ¬ä¸­çš„å•å¼•å·ç»Ÿä¸€è½¬æ¢ä¸ºåŒå¼•å·
    å°å¿ƒå¤„ç†è½¬ä¹‰å­—ç¬¦ï¼Œé¿å…ç ´åå­—ç¬¦ä¸²å†…å®¹
    
    Args:
        text: è¦æ ‡å‡†åŒ–çš„æ–‡æœ¬
        
    Returns:
        å¼•å·æ ‡å‡†åŒ–åçš„æ–‡æœ¬
    """
    result = []
    in_string = False
    string_char = None
    i = 0
    
    while i < len(text):
        c = text[i]
        
        if not in_string:
            if c == '"':
                in_string = True
                string_char = '"'
                result.append(c)
            elif c == "'":
                in_string = True
                string_char = "'"
                result.append('"')  # å•å¼•å·è½¬åŒå¼•å·
            else:
                result.append(c)
            i += 1
        else:
            # åœ¨å­—ç¬¦ä¸²å†…éƒ¨
            if c == '\\' and i + 1 < len(text):
                # è½¬ä¹‰å­—ç¬¦ï¼šä¿ç•™è½¬ä¹‰åºåˆ—
                result.append(c)
                result.append(text[i + 1])
                i += 2
            elif c == string_char:
                # å­—ç¬¦ä¸²ç»“æŸ
                in_string = False
                string_char = None
                result.append('"')  # ç»Ÿä¸€è½¬ä¸ºåŒå¼•å·
                i += 1
            else:
                result.append(c)
                i += 1
    
    return ''.join(result)


def extract_balanced_braces(text: str, start_pos: int = 0, normalize: bool = True) -> Optional[str]:
    """
    æå–ä» start_pos å¼€å§‹çš„å¹³è¡¡èŠ±æ‹¬å·å†…å®¹
    
    Args:
        text: è¦æœç´¢çš„æ–‡æœ¬
        start_pos: å¼€å§‹ä½ç½®
        normalize: æ˜¯å¦å…ˆæ ‡å‡†åŒ–å¼•å·ï¼ˆé»˜è®¤Trueï¼‰
        
    Returns:
        å¹³è¡¡çš„èŠ±æ‹¬å·å†…å®¹ï¼ˆåŒ…å«å¤–å±‚èŠ±æ‹¬å·ï¼‰ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å› None
    """
    # å…ˆå°†å•å¼•å·ç»Ÿä¸€è½¬æ¢ä¸ºåŒå¼•å·ï¼Œé¿å…æ··åˆå¼•å·å¯¼è‡´çš„è§£æé—®é¢˜
    if normalize:
        text = normalize_quotes(text)
    
    # ä»start_poså¼€å§‹æŸ¥æ‰¾ç¬¬ä¸€ä¸ª{
    brace_start = text.find('{', start_pos)
    if brace_start == -1:
        return None
    
    # è·Ÿè¸ªèŠ±æ‹¬å·çš„åµŒå¥—æ·±åº¦
    count = 0
    in_string = False
    escape_next = False
    
    for i in range(brace_start, len(text)):
        c = text[i]
        
        if escape_next:
            escape_next = False
            continue
        
        if c == '\\':
            escape_next = True
            continue
        
        # è·Ÿè¸ªå­—ç¬¦ä¸²çŠ¶æ€ï¼ˆç°åœ¨åªéœ€è¦å¤„ç†åŒå¼•å·ï¼‰
        if c == '"':
            in_string = not in_string
        elif not in_string:
            if c == '{':
                count += 1
            elif c == '}':
                count -= 1
                if count == 0:
                    # æ‰¾åˆ°åŒ¹é…çš„é—­åˆæ‹¬å·
                    return text[brace_start:i+1]
    
    # æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„é—­åˆæ‹¬å·
    return None


def parse_schema_string(schema_str: str) -> Optional[Dict]:
    """
    å°è¯•å¤šç§æ–¹æ³•è§£æ schema å­—ç¬¦ä¸²
    
    Args:
        schema_str: åŸå§‹ schema å­—ç¬¦ä¸²
        
    Returns:
        è§£æåçš„å­—å…¸ï¼Œå¤±è´¥è¿”å› None
    """
    if not schema_str or not schema_str.strip():
        return None
    
    # é¢„å¤„ç†ï¼šç§»é™¤é¦–å°¾ç©ºç™½
    schema_str = schema_str.strip()
    
    # ç­–ç•¥1: å…ˆæ ‡å‡†åŒ–å¼•å·ï¼Œå†ä½¿ç”¨fix_json_string + json.loads
    try:
        normalized = normalize_quotes(schema_str)
        fixed = fix_json_string(normalized)
        result = json.loads(fixed)
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºå­—å…¸ï¼ˆå¯èƒ½æ˜¯è§£æé”™è¯¯ï¼‰
        if result and isinstance(result, dict):
            return result
    except Exception as e:
        pass
    
    # ç­–ç•¥2: ast.literal_evalï¼ˆå¤„ç† Python dict å­—é¢é‡ï¼‰
    try:
        result = ast.literal_eval(schema_str)
        if result and isinstance(result, dict):
            return result
    except Exception as e:
        pass
    
    # ç­–ç•¥3: ç®€å•æ›¿æ¢åå°è¯•
    try:
        simple = schema_str.replace('None', 'null').replace('True', 'true').replace('False', 'false')
        simple = normalize_quotes(simple)
        result = json.loads(simple)
        if result and isinstance(result, dict):
            return result
    except Exception as e:
        pass
    
    # ç­–ç•¥4: ä½¿ç”¨fix_json_stringä½†ä¸æ ‡å‡†åŒ–å¼•å·ï¼ˆå¤„ç†å·²ç»æ˜¯åŒå¼•å·çš„æƒ…å†µï¼‰
    try:
        fixed = fix_json_string(schema_str)
        result = json.loads(fixed)
        if result and isinstance(result, dict):
            return result
    except Exception as e:
        pass
    
    return None


def check_field_loss(original_text: str, parsed_schema: Dict, tool_name: str, schema_type: str) -> List[str]:
    """
    æ£€æŸ¥åŸå§‹æ–‡æœ¬ä¸­çš„å…³é”®å­—æ®µæ˜¯å¦åœ¨è§£æç»“æœä¸­ä¸¢å¤±
    
    ç­–ç•¥ï¼š
    1. æå–åŸå§‹æ–‡æœ¬ä¸­å‡ºç°çš„æ‰€æœ‰å­—æ®µåï¼ˆåœ¨å¼•å·ä¸­çš„ï¼‰
    2. æ£€æŸ¥è¿™äº›å­—æ®µåæ˜¯å¦åœ¨è§£æåçš„schemaä¸­å­˜åœ¨
    3. åªæŠ¥å‘Šä¸¢å¤±çš„å­—æ®µ
    
    Args:
        original_text: åŸå§‹æ–‡æœ¬ç‰‡æ®µ
        parsed_schema: è§£æåçš„schema
        tool_name: å·¥å…·å
        schema_type: 'Parameters' æˆ– 'Output'
        
    Returns:
        ä¸¢å¤±å­—æ®µçš„è­¦å‘Šåˆ—è¡¨
    """
    warnings = []
    
    if not original_text or not parsed_schema:
        return warnings
    
    # æå–åŸå§‹æ–‡æœ¬ä¸­çš„æ‰€æœ‰å­—æ®µåï¼ˆåœ¨å¼•å·ä¸­çš„ï¼‰
    # åŒ¹é…æ¨¡å¼: "field_name" æˆ– 'field_name' åé¢è·Ÿç€ :
    field_pattern = r'["\']([a-zA-Z_][a-zA-Z0-9_]*)["\']\s*:'
    original_fields = set(re.findall(field_pattern, original_text))
    
    # é€’å½’æå–è§£æåçš„schemaä¸­çš„æ‰€æœ‰å­—æ®µå
    def extract_all_keys(obj, keys=None):
        if keys is None:
            keys = set()
        if isinstance(obj, dict):
            keys.update(obj.keys())
            for value in obj.values():
                extract_all_keys(value, keys)
        elif isinstance(obj, list):
            for item in obj:
                extract_all_keys(item, keys)
        return keys
    
    parsed_fields = extract_all_keys(parsed_schema)
    
    # æ‰¾å‡ºä¸¢å¤±çš„å­—æ®µ
    lost_fields = original_fields - parsed_fields
    
    # è¿‡æ»¤æ‰ä¸€äº›å¸¸è§çš„éå…³é”®å­—æ®µï¼ˆå¯èƒ½æ˜¯è¯¯æŠ¥ï¼‰
    # ä¾‹å¦‚ï¼šå¦‚æœåŸæ–‡æœ¬ä¸­æœ‰ "type":"string" ä½†è§£æåå˜æˆ type: "string"ï¼Œ
    # é‚£ä¹ˆ "string" ä¹Ÿä¼šè¢«åŒ¹é…ä¸ºå­—æ®µåï¼Œä½†å®é™…ä¸Šå®ƒæ˜¯å€¼
    common_values = {'string', 'integer', 'number', 'boolean', 'array', 'object', 'null',
                     'low', 'medium', 'high', 'complete', 'partial', 'true', 'false'}
    lost_fields = lost_fields - common_values
    
    if lost_fields:
        for field in sorted(lost_fields):
            warnings.append(f"å·¥å…· {tool_name} - {schema_type}.{field} - å­—æ®µåœ¨è§£æåä¸¢å¤±")
    
    return warnings


def validate_schema(schema: Dict, schema_type: str, tool_name: str) -> List[str]:
    """
    éªŒè¯ schema çš„å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§
    æ³¨æ„ï¼šæ­¤å‡½æ•°å·²åºŸå¼ƒï¼Œæ”¹ç”¨ compare_schemas è¿›è¡Œå¯¹æ¯”éªŒè¯
    
    Args:
        schema: è¦éªŒè¯çš„ schema å­—å…¸
        schema_type: 'Parameters' æˆ– 'Output'
        tool_name: å·¥å…·å
        
    Returns:
        è­¦å‘Šä¿¡æ¯åˆ—è¡¨
    """
    # æ­¤å‡½æ•°ä¿ç•™ä½†ä¸å†ä½¿ç”¨ï¼Œé¿å…ç ´åç°æœ‰ä»£ç 
    return []


def normalize_tool_txt(input_path: str, output_path: str, verbose: bool = True, debug: bool = False):
    """
    æ ‡å‡†åŒ– tool.txt æ–‡ä»¶
    
    Args:
        input_path: åŸå§‹ tool.txt è·¯å¾„
        output_path: è¾“å‡ºçš„æ ‡å‡†åŒ–æ–‡ä»¶è·¯å¾„
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯ï¼ˆåŒ…æ‹¬åŸå§‹æ–‡æœ¬ç‰‡æ®µï¼‰
        
    Returns:
        (æˆåŠŸæ ‡å¿—, è§£æé”™è¯¯åˆ—è¡¨)
    """
    with open(input_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æŒ‰å·¥å…·åˆ†å‰²ï¼ˆæ¯ä¸ªå·¥å…·ä»¥æ•°å­—+ç‚¹å¼€å¤´ï¼‰
    tool_blocks = re.split(r'\n(?=\d+\.\s+Name:)', content.strip())
    
    normalized_tools = []
    parse_errors = []
    parse_warnings = []
    validation_warnings = []
    debug_info = []  # å­˜å‚¨è°ƒè¯•ä¿¡æ¯
    
    for block in tool_blocks:
        if not block.strip():
            continue
        
        # æå–å·¥å…·ç¼–å·
        num_match = re.search(r'^(\d+)\.\s+Name:', block)
        tool_num = num_match.group(1) if num_match else '?'
        
        # æå–å·¥å…·å
        name_match = re.search(r'Name:\s*(\w+)', block)
        if not name_match:
            parse_errors.append(f"å·¥å…· {tool_num} - æ— æ³•æå–å·¥å…·å")
            continue
        tool_name = name_match.group(1)
        
        # æå–æè¿°ï¼ˆæ”¯æŒå¤šè¡Œï¼‰
        desc_match = re.search(r'Description:\s*(.+?)(?=\nParameters:|\nOutput:|$)', block, re.DOTALL)
        description = desc_match.group(1).strip() if desc_match else ''
        
        # æ¸…ç†æè¿°ä¸­çš„å¤šä½™ç©ºç™½
        description = ' '.join(description.split())
        
        # æå–å¹¶è§£æ Parameters
        params_schema = None
        params_pos = block.find('Parameters:')
        params_raw_text = ''  # è®°å½•åŸå§‹æ–‡æœ¬
        if params_pos != -1:
            # æå–Parametersåçš„åŸå§‹æ–‡æœ¬ï¼ˆç”¨äºè°ƒè¯•å’Œå¯¹æ¯”ï¼‰
            params_line_end = block.find('\n', params_pos)
            if params_line_end != -1:
                params_raw_text = block[params_pos:params_line_end].strip()
            else:
                params_raw_text = block[params_pos:].strip()
            
            # æå–æ ‡å‡†åŒ–åçš„æ–‡æœ¬
            params_str = extract_balanced_braces(block, params_pos, normalize=True)
            
            if params_str:
                # è§£ææ ‡å‡†åŒ–åçš„æ–‡æœ¬
                params_schema = parse_schema_string(params_str)
                
                if params_schema is None:
                    parse_errors.append(f"å·¥å…· {tool_num} ({tool_name}) - Parameters è§£æå¤±è´¥")
                    if debug:
                        debug_info.append(f"å·¥å…· {tool_num} ({tool_name}) - Parameters åŸå§‹æ–‡æœ¬: {params_raw_text}")
                        debug_info.append(f"  æå–çš„å†…å®¹: {params_str[:200]}...")
            else:
                parse_warnings.append(f"å·¥å…· {tool_num} ({tool_name}) - Parameters æœªæ‰¾åˆ°èŠ±æ‹¬å·")
                if debug:
                    debug_info.append(f"å·¥å…· {tool_num} ({tool_name}) - Parameters åŸå§‹æ–‡æœ¬: {params_raw_text}")
        
        # æå–å¹¶è§£æ Output
        output_schema = None
        output_pos = block.find('Output:')
        output_raw_text = ''  # è®°å½•åŸå§‹æ–‡æœ¬
        if output_pos != -1:
            # æå–Outputåçš„åŸå§‹æ–‡æœ¬ï¼ˆç”¨äºè°ƒè¯•å’Œå¯¹æ¯”ï¼‰
            output_line_end = block.find('\n', output_pos)
            if output_line_end != -1:
                output_raw_text = block[output_pos:output_line_end].strip()
            else:
                output_raw_text = block[output_pos:].strip()
            
            # æå–æ ‡å‡†åŒ–åçš„æ–‡æœ¬
            output_str = extract_balanced_braces(block, output_pos, normalize=True)
            
            if output_str:
                # è§£ææ ‡å‡†åŒ–åçš„æ–‡æœ¬
                output_schema = parse_schema_string(output_str)
                
                if output_schema is None:
                    parse_errors.append(f"å·¥å…· {tool_num} ({tool_name}) - Output è§£æå¤±è´¥")
                    if debug:
                        debug_info.append(f"å·¥å…· {tool_num} ({tool_name}) - Output åŸå§‹æ–‡æœ¬: {output_raw_text}")
                        debug_info.append(f"  æå–çš„å†…å®¹: {output_str[:200]}...")
                elif not output_schema or output_schema == {}:
                    # æ£€æµ‹é™é»˜å¤±è´¥ï¼šè§£ææˆåŠŸä½†ç»“æœä¸ºç©ºå­—å…¸
                    parse_warnings.append(f"å·¥å…· {tool_num} ({tool_name}) - Output è§£æä¸ºç©ºå­—å…¸")
                    if debug:
                        debug_info.append(f"å·¥å…· {tool_num} ({tool_name}) - Output åŸå§‹æ–‡æœ¬: {output_raw_text}")
                        debug_info.append(f"  æå–çš„å†…å®¹: {output_str[:200]}...")
            else:
                parse_warnings.append(f"å·¥å…· {tool_num} ({tool_name}) - Output æœªæ‰¾åˆ°èŠ±æ‹¬å·")
                if debug:
                    debug_info.append(f"å·¥å…· {tool_num} ({tool_name}) - Output åŸå§‹æ–‡æœ¬: {output_raw_text}")
                    # è¾“å‡ºOutput:åé¢çš„å†…å®¹ä¾›è¯Šæ–­
                    snippet = output_raw_text[7:].strip()[:100]  # è·³è¿‡'Output:'
                    debug_info.append(f"  Output: åçš„å†…å®¹: '{snippet}'")
        
        # æ„å»ºæ ‡å‡†åŒ–çš„å·¥å…·å®šä¹‰
        normalized_block = f"{tool_num}. Name: {tool_name}\n"
        normalized_block += f"Description: {description}\n"
        
        if params_schema:
            # ä½¿ç”¨ç´§å‡‘æ ¼å¼ï¼Œä½†ä¿æŒå¯è¯»æ€§
            normalized_block += f"Parameters: {json.dumps(params_schema, ensure_ascii=False, separators=(',', ': '))}\n"
        else:
            normalized_block += "Parameters: {}\n"
        
        if output_schema:
            normalized_block += f"Output: {json.dumps(output_schema, ensure_ascii=False, separators=(',', ': '))}\n"
        else:
            normalized_block += "Output: {}\n"
        
        normalized_tools.append(normalized_block)
        
        # æ£€æŸ¥åŸå§‹æ–‡æœ¬ä¸­çš„å­—æ®µæ˜¯å¦åœ¨è§£æåä¸¢å¤±
        if params_schema and params_raw_text:
            param_warnings = check_field_loss(params_raw_text, params_schema, tool_name, 'Parameters')
            validation_warnings.extend(param_warnings)
        
        if output_schema and output_raw_text:
            output_warnings = check_field_loss(output_raw_text, output_schema, tool_name, 'Output')
            validation_warnings.extend(output_warnings)
    
    # å†™å…¥æ ‡å‡†åŒ–æ–‡ä»¶
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(normalized_tools))
    
    # æ‰“å°æŠ¥å‘Š
    if verbose:
        print("=" * 60)
        print("æ ‡å‡†åŒ–å®Œæˆï¼")
        print("=" * 60)
        print(f"è¾“å…¥æ–‡ä»¶: {input_path}")
        print(f"è¾“å‡ºæ–‡ä»¶: {output_path}")
        print(f"å¤„ç†å·¥å…·æ•°: {len(normalized_tools)}")
        
        if parse_warnings:
            print(f"\nâš  è­¦å‘Š ({len(parse_warnings)} ä¸ª):")
            for warn in parse_warnings:
                print(f"  {warn}")
        
        if validation_warnings:
            print(f"\nâš  è´¨é‡æ£€æŸ¥è­¦å‘Š ({len(validation_warnings)} ä¸ª):")
            for warn in validation_warnings:
                print(f"  {warn}")
        
        if parse_errors:
            print(f"\nâœ— è§£æé”™è¯¯ ({len(parse_errors)} ä¸ª):")
            for err in parse_errors:
                print(f"  {err}")
        else:
            print("\nâœ“ æ‰€æœ‰å·¥å…· schema è§£ææˆåŠŸï¼")
        
        if debug and debug_info:
            print(f"\nğŸ” è°ƒè¯•ä¿¡æ¯ ({len(debug_info)} æ¡):")
            for info in debug_info:
                print(f"  {info}")
        
        print("=" * 60)
        
        if len(parse_errors) == 0:
            print(f"\nå»ºè®®ï¼š")
            print(f"  1. æ£€æŸ¥ {output_path} ç¡®è®¤æ ¼å¼æ­£ç¡®")
            print(f"  2. å¤‡ä»½åŸæ–‡ä»¶: cp {input_path} {input_path}.bak")
            print(f"  3. æ›¿æ¢åŸæ–‡ä»¶: cp {output_path} {input_path}")
            print(f"  æˆ–åœ¨å…¶ä»–è„šæœ¬ä¸­ç›´æ¥ä½¿ç”¨ {output_path}")
    
    return len(parse_errors) == 0, parse_errors


if __name__ == "__main__":
    input_file = "/Users/liaoying/Desktop/ç ”ä¸€/llm/data_eval/precision_index/tool.txt"
    output_file = "/Users/liaoying/Desktop/ç ”ä¸€/llm/data_eval/precision_index/tool_normalized.txt"
    
    # è®¾ç½® debug=True å¯ä»¥çœ‹åˆ°è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
    success, errors = normalize_tool_txt(input_file, output_file, verbose=True, debug=True)
    
    if not success:
        print("\néœ€è¦æ‰‹åŠ¨ä¿®å¤ä»¥ä¸‹é—®é¢˜åé‡æ–°è¿è¡Œ:")
        for err in errors:
            print(f"  - {err}")
        exit(1)
    else:
        print("\nâœ“ æ ‡å‡†åŒ–æˆåŠŸï¼")
        exit(0)
